{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SamuelHennon_Natural_Language_Processing_FakeNews .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9wUL_Fy5qUDI",
        "P0jDiHoVfNkx",
        "9UffZDIz2bBR",
        "8VOrD3uZ3Q8P",
        "u41CZ87okvTM",
        "0qNXWPSymn5G",
        "MZrJb8lM0AV6",
        "VcZMDf7dqJGY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "\n",
        "My name is Samuel Hennon and my type of model is based on MultiNomial Naive Bayes using Bayes thoerm.It calculates probability and was modeled after the article Fake News Detection using NLP techniques by Joyce Annie George.\n",
        "\n"
      ],
      "metadata": {
        "id": "xIoGtgMwQdvW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wUL_Fy5qUDI"
      },
      "source": [
        "#IMPORTED LIBRARIES\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9apbZGptej6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "ad0130d3-56ab-4ad8-bb6d-425109ec8deb"
      },
      "source": [
        "#Import libraries and connect to google drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9f24a55f1962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import model specific resources or libraries\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn.metrics as metrics                                                 \n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "z43fKB6Ey5nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdqXyFZ95P0j"
      },
      "source": [
        "#The basepath is the same as what it is in the class assignments for simplicity\n",
        "basePath = \"/content/drive/My Drive/Colab Notebooks/Artificial Intelligence/Data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data files used, and how the files are structured\n",
        "\n",
        "The Dataset used for this Machine Learning Model is a Kaggle Dataset consisting of Fake and Real news articles, formatted in 'csv' files. \n",
        "\n",
        "The link to the dataset is: https://www.kaggle.com/c/fake-news/data?select=test.csv\n",
        "\n",
        "Downloading the dataset will give you two file \"train.csv\" and \"test.csv\". For simplicities sake, the contents of both datasets will get combined into one larger data set in which the Model will be trained and tested with.\n",
        "\n",
        "As shown above, during the making of this Notebook the train and test csv files where loaded into the base path directory that was usued for all class assignment. This was done for consistancy with the class and for simplicity."
      ],
      "metadata": {
        "id": "P0jDiHoVfNkx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XoSqVJG5FkG"
      },
      "source": [
        "# Data file name variables\n",
        "train = basePath + \"train.csv\"\n",
        "test = basePath + \"test.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Delete\n",
        "train = \"train.csv\"\n",
        "test = \"test.csv\""
      ],
      "metadata": {
        "id": "u-F-uk74xgru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CSV files come foramtted with 5 total columns. The columns are 'id', 'title', 'author', 'text', and 'label'. \n",
        "\n",
        "    ID: The designation number of the article in the file\n",
        "    Title: Title of the associated news article\n",
        "    Author: Person who wrote the News article\n",
        "    Text: All words of the news article \n",
        "    Label: Designation of whether the news article is reliable or unreliable. O means unreliable, 1 means reliable. (These essentially refer to Fake or Real)\n",
        "\n",
        "'Train.csv' has 20799 rows in it by default. 'Test.csv' has 25999 rows in it by default."
      ],
      "metadata": {
        "id": "xF9DLSp5gK0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Formatting the Data\n",
        "\n",
        "In order to use data of the files, it has to be prepared and formatted in a certain manner. This invovles scrubbing the id, title, and author columns from the both datasets. Also, any rows with NAN values need to be removed entirely as they are unsuitable. \n",
        "\n",
        "The next step involves combining the contents of the train and test datasets into one final dataset that will be used for the Machine Learning portion of the notebook."
      ],
      "metadata": {
        "id": "9UffZDIz2bBR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l73PTZtCPdxj"
      },
      "source": [
        "# 0 = reliable, 1 = unreliable\n",
        "\n",
        "# Read the training file\n",
        "data_train = pd.read_csv(train)\n",
        "data_train = data_train.drop(['id','author','title'], 1)\n",
        "data_train.dropna( inplace=True) \n",
        "data_train.reset_index(inplace = True, drop = True)\n",
        "print( data_train.shape )\n",
        "\n",
        "# Read the testing data file\n",
        "data_test  = pd.read_csv( test )\n",
        "data_test = data_test.drop(['id','author','title'], 1)\n",
        "data_test.dropna( inplace=True) \n",
        "data_test = data_test.loc[ :, :'label']\n",
        "data_test.reset_index(inplace = True, drop = True)\n",
        "print( data_test.shape )\n",
        "\n",
        "# Combine both train and test datasets into a final dataset which is used later on\n",
        "dataset = pd.concat( [data_train, data_test], ignore_index=True)\n",
        "print( dataset.shape)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check the final dataset to make sure it is valid for usage\n",
        "\n",
        "The final dataset that will be used has 2 columns of \"text\" and \"label\". There are about 20762 total rows in the dataset, with about a 50:50 percent split between Fake and Real articles in the set. These statistics are shown below. "
      ],
      "metadata": {
        "id": "8VOrD3uZ3Q8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#graph the total number of fake and real articles in the final dataset\n",
        "real_or_not = dataset.label.value_counts()\n",
        "print( real_or_not )\n",
        "\n",
        "plt.figure()\n",
        "plt.bar( ['0','1'], real_or_not )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SVf_Jw4AhduN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is lastly checked so that no NaN values are present. The ouptut of the next cell is empty because there are no NaN values present."
      ],
      "metadata": {
        "id": "ip32DkYHkalb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.dropna(inplace=True)\n",
        "dataset[dataset.isnull().any(axis=1)]"
      ],
      "metadata": {
        "id": "AAoZoWt7HK9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Supervised Machine Learning \n",
        "\n",
        "From this point on is where the actual Machine Learning and predictions of the Notebook will occur.\n",
        "\n",
        "The next cell shows a helpful function to display the results of the machine learning in a discenrable manner, taken from this source.\n",
        "\n"
      ],
      "metadata": {
        "id": "u41CZ87okvTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A helpful function pulled from a website (credited at the end of this Notebook) that will print out the results of the Models processing in a concise and understandable manner.\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                  horizontalalignment=\"center\",\n",
        "                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "8hYsvFoF1Rgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the dataset into testing and training\n",
        "The next cell shows the dataset being partitioned into training, and testing sets ( 80% for training, 20% for testing ) that will be used for the Model."
      ],
      "metadata": {
        "id": "0qNXWPSymn5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = dataset['label']\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split( dataset['text'], y, test_size=0.2, random_state=7)"
      ],
      "metadata": {
        "id": "JR5RT78FnDAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model\n",
        "\n",
        "I am using a model called MultiNomial Naive Bayes model it uses bayes theorm that uses a naieve assumtion that all items are different and non related to eachother. This model was influenced by the mentioned article from Joyce Annie George the article is in the reference section at the end of the document."
      ],
      "metadata": {
        "id": "MZrJb8lM0AV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "xtrain,xtest,ytrain,ytest = train_test_split(data_train['text'],data_train['label'],test_size=0.2, random_state = 1)\n",
        "\n",
        "data_train = data_train.fillna('')\n",
        "data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "data_train.fillna(999, inplace=True)\n"
      ],
      "metadata": {
        "id": "N04uegUG6yWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = data_train.fillna('')\n",
        "pipe = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "model = pipe.fit(xtrain, ytrain)\n",
        "prediction = model.predict(xtest)\n"
      ],
      "metadata": {
        "id": "QC9XUyGf0Uxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Results and Conclusions\n",
        "This method is very effective with a accuracy of roughly 85% as shown in the plot out below."
      ],
      "metadata": {
        "id": "WUqBXFQ2AMZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = metrics.accuracy_score(ytest, prediction)\n",
        "print(\"accuracy:   %0.3f\" % (score*100))\n",
        "cm = metrics.confusion_matrix(ytest, prediction, labels=[0,1])\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(ytest, prediction),\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                colorbar=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F60WWbFayVQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I lerned that the  MultiNomial Naive Bayes is an effective method that had a decent mix of accuracy and training speed."
      ],
      "metadata": {
        "id": "VcZMDf7dqJGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sources and References used:\n",
        "\n",
        "https://iopscience.iop.org/article/10.1088/1742-6596/2007/1/012047/pdf\n",
        "\n",
        "https://www.kaggle.com/c/fake-news/data?select=test.csv\n",
        "\n",
        "https://towardsdatascience.com/detecting-fake-political-news-online-a571745f73dd\n",
        "\n",
        "https://medium.com/analytics-vidhya/fake-news-detection-using-nlp-techniques-c2dc4be05f99\n",
        "\n",
        "https://en.wikipedia.org/wiki/Bayes%27_theorem#:~:text=In%20probability%20theory%20and%20statistics,be%20related%20to%20the%20event.\n",
        "\n"
      ],
      "metadata": {
        "id": "NqAdvweJ5W30"
      }
    }
  ]
}
